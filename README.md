Repository of the practical part of thesis about loop unroll factor choice in hardware design and software compilation. The main framework for LLVM comes from the Compiler Construction course: https://bitbucket.org/vusec/vu-coco-public/src/master/. It is primarly used in the hardware part but it also allows for consistency with LLVM version and compilation.

The repository contains the framework and inside there are three main directories inside `framework/LUF`: `hardware_ML`, `software_ML` and `software_clang`, which represent the main three parts of the thesis. 

Hardware part relies on the ALADDIN tool from https://github.com/harvard-acc/ALADDIN for the latency and area measurements. It can be installed the simplest as a docker container from https://hub.docker.com/repository/docker/xyzsam/gem5-aladdin, installation is described in the repository. Relevant parts for this are in the `framework/LUF/hardware_ML/gather_latency_area/` directory. In order to replicate the results: after cloning the ALADDIN repository into the docker container, you need to create a folder in each directory of a benchmark (for each benchmark directory in `MachSuite` and `SHOC` directories) called `example` and add respective config file from `config_files` to it, renaming it to `config_example`. Finally, add the `run_unrolls.py` file to each directory of a benchmark and run it (all still inside the docker container). This will run the latency/area tests and provide results for each loop line by line in `export_<program_name>.csv` in the `example` directory you have created before.

For getting the loop metrics, the relevant parts are in the `framework/LUF/hardware_ML/gather_loops_metrics` directory. You can either use the precompiled files that are already there or you can also recompile them using `compile_hardware.py` (it is best to compile it inside the docker container for consistency and no clang/LLVM version issues) and then get the metrics using `gather_loops_metrics.py`. This requires the Compiler Construction framework installed - do so by going to `framework` directory and run `run.sh` (requires docker installed and running). Afterwards, run `make passes` and now you can gather the metrics or use the tool on a single .ll file to see how it works, for example: `myopt -loop-analysis pp_scan/pp_scan.ll` (done from `framework/LUF/hardware_ML/gather_loops_metrics`). The results are printed above the code in the console. It is worth noting it prints all the loops, not only the ones that were used for the measurements. It prints them in a specific order and to be sure which loop you are looking for in the code, use for example `myopt -loop-analysis pp_scan/pp_scan_dbg.ll` to see at which line exactly the loop resides (other metrics given here may not be accurate, the former command is used for this).

Finally, in the `framework/LUF/hardware_ML` directory, we combine the results. The `LUF_predictor.py` needs the `loops_data.csv` (the file with all measurement data in a specific layout that were copied from `framework/LUF/hardware_ML/gather_loops_metrics/metrics_export.txt`) and can be run to train the models. `Results.xlsx` file is a cleaner display of this data.

Software parts rely on hyperfine tool from https://github.com/sharkdp/hyperfine. The relevant parts are in the `framework/LUF/software_ML` and `framework/LUF/software_clang` directories. Firstly install hyperfine. Then in both directories, `software_ML` and `software_clang`, you can use the already compiled code or recompile it using `compile_software_ML.py`/`compile_software_clang.py` (it is best to compile it inside the docker container for consistency and no clang/LLVM version issues). Then, run the `run_hyperfine_ML.py`/`run_hyperfine_clang.py` (run sudo -v beforehand) to obtain the data (already present in each benchmark in `compile`/`unroll_no_auto_all` directory as the hyperfine files). Anything related to hyperfine you can run on your main machine or wherever you installed it. Finally, run `hyperfine_analysis_ML.py`/`hyperfine_analysis_clang.py` to obtain whiskers (already present in the directories under `whiskers`). 

References: 
- Aladdin: A Pre-RTL, Power-Performance Accelerator Simulator Enabling Large Design Space Exploration of Customized Architectures, Yakun Sophia Shao, Brandon Reagen, Gu-Yeon Wei and David Brooks, International Symposium on Computer Architecture, June, 2014
- Peter, D. (2023). hyperfine (Version 1.16.1) [Computer software]. https://github.com/sharkdp/hyperfine
